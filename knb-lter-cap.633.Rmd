---
title: "capeml template"
author: "information manager"
date: Sys.Date()
output: html_document
editor_options: 
  chunk_output_type: console
---

# libraries

```{r libraries}
library(EML)
library(tidyverse)
library(capeml)
library(gioseml)
library(yaml)
```
recodeTransect: function to standardize tsect names, these will be passed to
the tibble flow for each datasets. Input is a string or vector of strings (from
Tibble), output is a string or vector of strings standarized to (1) upper-case
alpha, (2) trimmed of white space, (3) underscore separating each meaningful
characters.

```{r tsect-recode, eval=TRUE}

recodeTransect <- function(unformatted_tsect) {

  # uppercase and trim white space
  trimmed <- toupper(str_trim(unformatted_tsect, side = c("both")))

  # remove any existing character separators
  separator <- gsub("-|:| ", "", trimmed)

  # convert all tsets to format char_char
  nosep <- gsub("_", "", separator)
  underscore <- gsub("(.)\\B", "\\1_", nosep)

  # return standardized tsect
  return(underscore)

}

```

# typha_leaves

```{r typha-leaves, eval=TRUE}

primary_productivity <- read_csv("raw_data/tr_agpp_data.csv")

primary_productivity[primary_productivity == ""] <- NA

primary_productivity <- primary_productivity %>%
  mutate(
    date = as.POSIXct(date, format = "%m/%d/%y"),
    date = format(date, "%Y-%m-%d"),
    transect = recodeTransect(transect),
    species = as.factor(species),
    plant_id = seq_along(date),
    data_book_ID = as.character(`data book ID`),
    quadrat = as.character(quadrat),
    species = replace(species, species == "s_cal", "s_californicus")
  )

# typha

typha_leaves <- primary_productivity %>%
  select(date:species, plant_id, data_book_ID, Notes, starts_with("leaf")) %>%
  gather(key = "leaf_num", value = "leaf_length", starts_with("leaf")) %>%
  filter(!is.na(leaf_length)) %>%
  mutate(
    plant_id = as.character(plant_id),
    leaf_length = as.numeric(leaf_length)
    ) %>%
  arrange(plant_id) %>%
  as.data.frame

# write_attributes(typha_leaves)
# write_factors(typha_leaves)

typha_leaves_desc <- "Tres Rios: indices of primary productivity. Measurements of lenghts of Typha spp. leaves located within study transects at the Tres Rios wetlands. These data are used as inputs into biomass models to calculate above-ground primary productivity"

typha_leaves_DT <- create_dataTable(
  dfname = typha_leaves,
  description = typha_leaves_desc
)

```

# plant_attributes

```{r plant-attributes, eval=TRUE}

plant_attributes <- primary_productivity %>%
  select(-starts_with("leaf")) %>%
  arrange(plant_id) %>%
  mutate(
    plant_id = as.character(plant_id),
    `data book ID` = as.character(`data book ID`)
    ) %>%
  select(date:species, plant_id, cdb:Notes) %>%
  rename(data_book_ID = `data book ID`) %>%
  as.data.frame()

# write_attributes(plant_attributes)
# write_factors(plant_attributes)

plant_attributes_desc <- "Tres Rios: indices of primary productivity. Measurements of various physiological characteristics of individual plants located within study transects at the Tres Rios wetlands. These data are used as inputs into biomass models to calculate above-ground primary productivity."

plant_attributes_DT <- create_dataTable(
  dfname = plant_attributes,
  description = plant_attributes_desc
)

```

# transpiration

```{r transpiration, eval=TRUE}

transpiration <- read_csv("raw_data/tr_irga_fielddata.csv")

transpiration[transpiration == ""] <- NA

transpiration <- transpiration %>%
  mutate(
    date_time = as.POSIXct(paste(date, time), format = "%m/%d/%y %H:%M:%S"),
    transect = case_when(
      transect == "boardwalk" ~ "boardwalk",
      transect != "boardwalk" ~ recodeTransect(transect)
      ),
    obs_num = as.character(obs_num),
    plant_spp = replace(plant_spp, plant_spp == "stab", "s_tabernaemontani"),
    plant_spp = replace(plant_spp, plant_spp == "sac", "s_acutus"),
    plant_spp = replace(plant_spp, plant_spp == "smar", "s_maritimus"),
    plant_spp = replace(plant_spp, plant_spp == "sam", "s_americanus"),
    plant_spp = replace(plant_spp, plant_spp == "scal", "s_californicus"),
    plant_spp = replace(plant_spp, plant_spp == "sac/stab", "s_acutus_tabernaemontani"),
    plant_spp = replace(plant_spp, plant_spp == "sac/stad", "s_acutus_tabernaemontani"),
    plant_spp = replace(plant_spp, plant_spp == "sacstab", "s_acutus_tabernaemontani"),
    plant_spp = replace(plant_spp, plant_spp == "hyd", "h_umbellate"),
    plant_spp = replace(plant_spp, plant_spp == "typ", "typha"),
    plant_spp = replace(plant_spp, plant_spp == "hyb", "h_umbellate"),
    plant_spp = replace(plant_spp, plant_spp == "tlat", "typha"),
    plant_spp = as.factor(plant_spp)
    ) %>%
filter(!is.na(date_time)) %>%
rename(species = plant_spp) %>%
select(date_time, obs_num:VpdA) %>%
as.data.frame()

# write_attributes(transpiration)
# write_factors(transpiration)

transpiration_desc <- "Tres Rios: transpiration. Measurements of leaf-level gas exchange and micro-climate taken using an infrared gas analyzer on individual plant leaves located within study transects at the Tres Rios wetlands. These instantaneous micro-climate and transpiration data are used as inputs into transpiration and evaporation models that scale these data and estimate whole-system atmospheric water losses."

transpiration_DT <- create_dataTable(
  dfname = transpiration,
  description = transpiration_desc
)

```

# water_quality

```{r water-quality, eval=TRUE}

water_quality <- read_csv("raw_data/tr_waterquality_fielddata.csv")

water_quality[water_quality == ""] <- NA

water_quality <- water_quality %>%
  mutate(
    date = as.POSIXct(date, format = "%m/%d/%y"),
    date = format(date, "%Y-%m-%d"),
    transect = case_when(
      grepl("m", transect) ~ recodeTransect(transect),
      transect != "m_1_e|m_4_n|m_4_s" ~ transect
    ),
    location = as.factor(location)
  ) %>%
  select(-sample_id, -doc_id) %>%
  as.data.frame()

# write_attributes(water_quality)
# write_factors(water_quality)

water_quality_desc <- "Tres Rios: water_quality. Measurements of water quality taken in situ and from grab samples collected within study transects at the Tres Rios wetlands."

water_quality_DT <- create_dataTable(
  dfname = water_quality,
  description = water_quality_desc
)

```

# people

See the gioseml package for examples of creating people resources from scratch.

```{r people}

# creator(s) - required

dan <- create_role(
  firstName = "d",
  lastName = "childers",
  roleType = "creator")
dax <- create_role(
  giosPersonId = 31196,
  roleType = "creator")

creators <- list(dan, dax)

# metadata provider - required

chris <- create_role(
  giosPersonId = 16181,
  roleType = "metadata")
dax <- create_role(
  giosPersonId = 31196,
  roleType = "metadata")

metadataProvider <- list(chris, dax)

# associated party - optional

# takes the optional argument `projectRole` (default: "former project
# associate")

chris <- create_role(
  giosPersonId = 16181,
  roleType = "associated")
nich <- create_role(
  firstName = "n",
  lastName = "weller",
  roleType = "associated")

associatedParty <- list(chris, nich)

```

# keywords

```{r keywords}

# CAP IRTs for reference (be sure to include these as appropriate):
# https://sustainability.asu.edu/caplter/research/

write_keywords()
```

# coverages

```{r coverages}

begindate <- as.character(min(
  min(primary_productivity$date),
  min(water_quality$date)
))

enddate <- as.character(max(
  max(primary_productivity$date),
  max(water_quality$date)
))

geographicDescription <- "CAP LTER study area"

coverage <- set_coverage(
  begin = begindate,
  end = enddate,
  geographicDescription = geographicDescription,
  west = -111.949, east = -111.910,
  north = +33.437, south = +33.430)

```

## taxonomic coverage

Taxonomic coverage(s) are constructed using EDI's taxonomyCleanr tool suite.

*Note* that the `taxa_map.csv` built with the `create_taxa_map()` function and
resolving taxonomic IDs (i.e., `resolve_comm_taxa()`) only needs to be run once,
a potentially long process, per version/session -- the taxonomicCoverage can be
built as many times as needed with `resolve_comm_taxa()` once the `taxa_map.csv`
has been generated and the taxonomic IDs resolved.

```{r taxonomyCleanr, eval=FALSE}

library(taxonomyCleanr)

my_path <- getwd() # taxonomyCleanr requires a path (to build the taxa_map)

# plant taxa listed in the om_transpiration_factors file

plantTaxa <- rbind(
  read_csv("plant_attributes_factors.csv"),
  read_csv("transpiration_factors.csv"),
  read_csv("typha_leaves_factors.csv")
  ) %>%
distinct(definition) %>%
as.data.frame()

# create or update map. A taxa_map.csv is the heart of taxonomyCleanr. This
# function will build the taxa_map.csv and put it in the path identified with
# my_path.
create_taxa_map(path = my_path, x = plantTaxa, col = "definition")

# Resolve taxa by attempting to match the taxon name (data.source 3 is ITIS but
# other sources are accessible). Use `resolve_comm_taxa` instead of
# `resolve_sci_taxa` if taxa names are common names but note that ITIS
# (data.source 3) is the only authority taxonomyCleanr will allow for common
# names.
resolve_sci_taxa(path = my_path, data.sources = 3) # in this case, 3 is ITIS

# build the EML taxonomomic coverage
taxaCoverage <- make_taxonomicCoverage(path = my_path)

# add taxonomic to the other coverages
coverage$taxonomicCoverage <- taxaCoverage
```

# dataset

Optionally, provide: scope, abstract, methods, keywords, publication date.
Projects scopes include lter (default), urex, ltreb, and som.

```{r construct-dataset}

dataset <- create_dataset()
```

# add dataTable

```{r dataSet$dataTable}

# add dataTables if relevant

print(ls(pattern = "_DT"))

if (length(ls(pattern = "_DT")) > 0) {

  listOfDataTables <- lapply(ls(pattern = "_DT"), function(DT) { get(DT) } )

  dataset$dataTable  <- listOfDataTables

}

# or add manually
# dataset$dataTable <- list(dataTableOne, dataTableTwo)

```


# customUnits

```{r custom-units, eval=FALSE}

custom_units <- rbind(
  data.frame(id = "micromolesPerMeterSquaredPerSecond",
    parentSI = "molesPerMeterSquaredPerSecond",
    unitType = "arealAmountOfSubstanceConcentrationRate",
    multiplierToSI = "1000000",
    description = ""),
  data.frame(id = "molesPerMeterSquaredPerSecond",
    parentSI = "molesPerMeterSquaredPerSecond",
    unitType = "arealAmountOfSubstanceConcentrationRate",
    multiplierToSI = "1",
    description = "unknown"),
  data.frame(id = "micromolesPerMole",
    parentSI = "",
    unitType = "unknown",
    multiplierToSI = "unknown",
    description = "unknown"),
  data.frame(id = "millimolesPerMeterSquarePerSecond",
    parentSI = "molesPerMeterSquaredPerSecond",
    unitType = "arealAmountOfSubstanceConcentrationRate",
    multiplierToSI = "unknown",
    description = "unknown"),
  data.frame(id = "micromolesPerSecond",
    parentSI = "molePerSecond",
    unitType = "mole flow rate",
    multiplierToSI = "unknown",
    description = "unknown"),
  data.frame(id = "microsiemens",
    parentSI = "siemens",
    unitType = "electrical conductance",
    multiplierToSI = "1.0E-6",
    description = "microsiemens is equivalent to 1.0E-6 siemens, the SI derived unit of electric conductance equal to inverse ohm"),
  data.frame(id = "microsiemensPerCentimeter",
    parentSI = "siemensPerMeter",
    unitType = "electrical conductance per length",
    multiplierToSI = "0.0001",
    description = "derivative of the standard SI unit of electrical conductance (specific conductance)")
)

unitList <- set_unitList(
  custom_units,
  as_metadata = TRUE)

```

# eml

```{r construct_eml, eval=TRUE}

eml <- create_eml()
```

```{r validate_eml, eval=TRUE}

eml_validate(eml)
```

```{r eml_to_file, eval=TRUE}

# write the eml to file
write_cap_eml()
```

# file placement

```{r package-details, eval=TRUE}

# retrieve package details from config.yaml
if (!file.exists("config.yaml")) {
  stop("config.yaml not found")
}
packageIdent <- yaml::yaml.load_file("config.yaml")$packageIdent
packageNum <- yaml::yaml.load_file("config.yaml")$packageNum
```

```{r preview_data_file_to_upload}

# preview data set files that will be uploaded to S3
list.files(pattern = paste0(packageNum, "_"))
```

Move data and final xml files to respective ASU locations.

```{r S3_helper_functions}
# functions and setting for uploading to S3
library(aws.s3)
source("~/Documents/localSettings/aws.s3")
```

```{r upload_data_S3}

# upload files to S3
lapply(list.files(pattern = paste0(packageNum, "_")), data_to_amz)
```

```{r clean_up}

# remove data files
dataFilesToRemove <- dir(pattern = paste0(packageNum, "_"))
file.remove(dataFilesToRemove)

# EML to S3
if(length(list.files(pattern = "*.xml")) == 1) {
  eml_to_amz(list.files(pattern = "*.xml")) } else {
    print("more than one xml file found")
  }

# EML to cap-data-eml and remove file from project
tryCatch({
  
  if(length(list.files(pattern = "*.xml")) == 1) {
    file.copy(list.files(pattern = "*.xml"), "/home/srearl/localRepos/cap-metadata/cap-data-eml/")
    file.remove(list.files(pattern = "*.xml")) } else {
      print("more than one xml file found")
    }
},
warning = function(warn) {
  print(paste("WARNING: ", warn))
},
error = function(err) {
  print(paste("ERROR: ", err))
  
}) # close try catch
```
